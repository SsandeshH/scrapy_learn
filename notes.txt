Scrapy Framework

pip install scrapy

Start a project:
scrapy startproject <name of project>

folder Structure:
projectname
    -spiders #where we write our crawlers
        - __init__.py
        - crawl-file.py
    
    - items.py #contains items of webpages, like product_name, product_price which we select while crawling
    - pipelines.py # Storing configuration, SQL,JSON,NoSQL etc
    - middlewares.py #Adding stuffs on request, like in user_agent . adding proxy happpens in middlewares
    - settings.py #contains configuration for crawler

Use of Item:
We work on dictionaries, but it is inconsitent and unstructured in larger project and Spiders.
Scraped Data ---> Temporary Container (Items) ----> Database

So create items on items.py , import it into your crawler and use items to temporarily store the data.

we can output the data in json,xml,csv
scrapy crawl -o data.csv or .xml or .json

If we have to send the data to SQL or NoSQL db, we have to use Pipelines
First we "uncomment" Item_pipelines from settings.py to enable pipelining. We can also add more pielines. The lower the number in pipeline, higher the priority


